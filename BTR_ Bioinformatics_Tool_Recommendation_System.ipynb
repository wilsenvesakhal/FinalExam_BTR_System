{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD ALL WORKFLOW DATA\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import constants\n",
    "\n",
    "# Description:\n",
    "# This function is used to download all public workflows \n",
    "# from a website and each workflow will be saved in the data\\\\raw_galaxy_files folder \n",
    "# with the .ga extension. If the workflow has already been downloaded, \n",
    "# it will not be downloaded again.\n",
    "def download(source):\n",
    "    # Download all public/shared workflows from the given usegalaxy source\n",
    "    out_dir = os.path.join(constants.RAW_GALAXY_FILES, source.replace(\".\", \"_\"))\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Don't download workflows that are already cached\n",
    "    existing_files = [os.path.splitext(file)[0] for file in os.listdir(out_dir)]\n",
    "    \n",
    "    # Get list of workflows from server\n",
    "    workflow_list_url = f\"https://{source}/api/workflows\"\n",
    "    print(\"Downloading workflow list from \", workflow_list_url)\n",
    "    workflow_list = requests.get(workflow_list_url).json()\n",
    "    \n",
    "    to_download = []\n",
    "    for workflow in workflow_list:\n",
    "        if workflow[\"id\"] not in existing_files:\n",
    "            to_download.append(workflow[\"id\"])\n",
    "    \n",
    "    print(f\"Downloading {len(to_download)} workflows from {source}\")\n",
    "    \n",
    "    # Download with respect to the API wait time specified in constants\n",
    "    prev_time = 0\n",
    "    num = 0\n",
    "    for workflow_id in to_download:\n",
    "        num += 1\n",
    "        \n",
    "        download_url = f\"https://{source}/api/workflows/{workflow_id}/download\"\n",
    "        \n",
    "        while (time.time() - prev_time < constants.GALAXY_API_WAIT):\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        prev_time = time.time()\n",
    "        print(f\"Downloading {num}/{len(to_download)}: {workflow_id}\")\n",
    "        r = requests.get(download_url)\n",
    "        \n",
    "        if r.status_code != 200:\n",
    "            print(f\"Error downloading {workflow_id}\")\n",
    "            print(\"Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(out_dir, f\"{workflow_id}.ga\"), \"w\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "# Description:\n",
    "#download all public workflow for each website\n",
    "def run():\n",
    "    print(\"Downloading workflows from AllGalaxy sources\")\n",
    "    with open(constants.WORKFLOW_SOURCES) as f:\n",
    "        sources = f.read().splitlines()\n",
    "    \n",
    "    for source in sources:\n",
    "        download(source)\n",
    "    \n",
    "    print(\"Done downloading workflows\")\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf09559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARE EACH WORKFLOW \n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import constants\n",
    "import utils\n",
    "\n",
    "\n",
    "#Description :\n",
    "# This function is used to process nodes and edges for each workflow if step[\"type\"] == \"subworkflow\"\n",
    "def process_subworkflow(subworkflow, nodes, edges, id_map, next_id):\n",
    "    sub_nodes = []\n",
    "    sub_edges = {}\n",
    "\n",
    "    sub_id_map = {}\n",
    "    sub_next_id = 0\n",
    "\n",
    "    for step in list(subworkflow[\"subworkflow\"][\"steps\"]):\n",
    "        step = subworkflow[\"subworkflow\"][\"steps\"][step]\n",
    "        sub_nodes, sub_edges, sub_id_map, sub_next_id = process_step(step, sub_nodes, sub_edges, sub_id_map, sub_next_id)\n",
    "    \n",
    "    nodes.extend(sub_nodes)\n",
    "\n",
    "    for from_sub_edge, to_sub_edges in sub_edges.items():\n",
    "        from_sub_edge += next_id\n",
    "        for to_sub_edge in to_sub_edges:\n",
    "            to_sub_edge += next_id\n",
    "            if from_sub_edge not in edges:\n",
    "                edges[from_sub_edge] = [to_sub_edge]\n",
    "            else:\n",
    "                edges[from_sub_edge].append(to_sub_edge)\n",
    "    \n",
    "    id_map[subworkflow[\"id\"]] = next_id + sub_next_id - 1\n",
    "\n",
    "    for in_connection in list(subworkflow[\"input_connections\"]):\n",
    "        in_connection = subworkflow[\"input_connections\"][in_connection]\n",
    "\n",
    "        if type(in_connection) != list:\n",
    "            in_connection = [in_connection]\n",
    "        for connection in in_connection:\n",
    "            from_id = id_map[connection[\"id\"]]\n",
    "            to_id = connection[\"input_subworkflow_step_id\"] + next_id\n",
    "            if from_id not in edges:\n",
    "                edges[from_id] = [to_id]\n",
    "            else:\n",
    "                edges[from_id].append(to_id)\n",
    "    \n",
    "    next_id += sub_next_id\n",
    "\n",
    "    return nodes, edges, id_map, next_id\n",
    "\n",
    "\n",
    "#Description :\n",
    "# This function is used to process nodes and edges for each workflow.\n",
    "def process_step(step, nodes, edges, id_map, next_id):\n",
    "    curr_step = {}\n",
    "\n",
    "    this_id = next_id\n",
    "    \n",
    "    # Extract relevant information from step to build node\n",
    "    curr_step[\"type\"] = step[\"type\"]\n",
    "    if \"tool_state\" in step and step[\"tool_state\"] is not None:\n",
    "        curr_step[\"tool_state\"] = step[\"tool_state\"]\n",
    "    curr_step[\"annotation\"] = step[\"annotation\"]\n",
    "    if \"label\" in step and step[\"label\"] is not None:\n",
    "        curr_step[\"label\"] = step[\"label\"]\n",
    "    curr_step[\"name\"] = step[\"name\"]\n",
    "    \n",
    "    # Galaxy workflows can contain subworkflows (that may contain subworkflows, etc.) so we need to recursively process them\n",
    "    if step[\"type\"] == \"subworkflow\":\n",
    "        nodes, edges, id_map, next_id = process_subworkflow(step, nodes, edges, id_map, next_id)\n",
    "    else:\n",
    "        # We have a single step, associate the in-edges\n",
    "        for in_connection in list(step[\"input_connections\"]):\n",
    "            in_connection = step[\"input_connections\"][in_connection]\n",
    "            \n",
    "            if type(in_connection) != list:\n",
    "                in_connection = [in_connection]\n",
    "            for connection in in_connection:\n",
    "                from_id = id_map[connection[\"id\"]]\n",
    "                if from_id not in edges:\n",
    "                    edges[from_id] = [this_id]\n",
    "                else:\n",
    "                    edges[from_id].append(this_id)\n",
    "        \n",
    "        # Get tool information\n",
    "        if step[\"type\"] != \"data_input\" and step[\"type\"] != \"data_collection_input\" and step[\"type\"] != \"parameter_input\":\n",
    "            curr_step[\"tool_id\"] = step[\"tool_id\"]\n",
    "            if \"tool_shed_repository\" in step:\n",
    "                curr_step[\"tool_name\"] = step[\"tool_shed_repository\"][\"name\"]\n",
    "                curr_step[\"tool_shed\"] = step[\"tool_shed_repository\"][\"tool_shed\"]\n",
    "            \n",
    "        nodes.append(curr_step)\n",
    "        id_map[step[\"id\"]] = this_id\n",
    "        next_id += 1\n",
    "    \n",
    "    return nodes, edges, id_map, next_id\n",
    "\n",
    "#Description :\n",
    "# This function serves to create a dictionary containing name, \n",
    "# tags, license, node, server, and workflow ID information for each workflow.\n",
    "def process_file(file_path):\n",
    "    workflow = utils.load_json(file_path)\n",
    "    \n",
    "    # Associate relevant information\n",
    "    name = workflow[\"name\"]\n",
    "    tags = workflow[\"tags\"]\n",
    "    license = workflow[\"license\"] if \"license\" in workflow else None\n",
    "    split_path = os.path.split(file_path)\n",
    "    workflow_id = split_path[1].split(\".\")[0]\n",
    "    server = os.path.split(split_path[0])[1]\n",
    "\n",
    "    nodes = []\n",
    "    edges = {}\n",
    "\n",
    "    id_map = {}\n",
    "    next_id = 0\n",
    "    \n",
    "    # Process each step to build list of nodes and edges\n",
    "    try:\n",
    "        for step in list(workflow[\"steps\"]):\n",
    "            step = workflow['steps'][step]\n",
    "            nodes, edges, id_map, next_id = process_step(step, nodes, edges, id_map, next_id)\n",
    "    except:\n",
    "        print(f\"Error processing file: {file_path}  Step: {step['id']}\")\n",
    "        \n",
    "    \n",
    "    return {\"name\": name, \"tags\": tags, \"license\": license, \"nodes\": nodes, \"edges\": edges, \"server\": server, \"workflow_id\": workflow_id}\n",
    "\n",
    "\n",
    "#Description :\n",
    "# This function is used to create json containing information \n",
    "# data for each workflow. Each workflow will have data consisting of name, \n",
    "# tags, license, node, server, and workflow ID. \n",
    "# The node contains step sequence data, \n",
    "# which consists of type, tool_state, annotation, \n",
    "# label, name, tool_id, tool_name, tool_shed information.\n",
    "# edges contain the relationships between steps. A step can have one or more than one input.\n",
    "\n",
    "def run():\n",
    "    workflows = []\n",
    "    num_nodes = 0\n",
    "    num_edges = 0\n",
    "    print(\"Processing Galaxy files for AllGalaxy dataset...\")\n",
    "    galaxy_files = utils.get_files_of_ext(constants.RAW_GALAXY_FILES, \"ga\")\n",
    "    for file in tqdm.tqdm(galaxy_files):\n",
    "        print(file)\n",
    "        workflow = process_file(file)\n",
    "        num_nodes += len(workflow[\"nodes\"])\n",
    "        for from_edge, to_edges in workflow[\"edges\"].items():\n",
    "            num_edges += len(to_edges)\n",
    "        workflows.append(workflow)\n",
    "    \n",
    "    utils.dump_json(workflows, os.path.join(constants.PROCESSED_WORKFLOWS_LOC, \"all_galaxy.json\"))\n",
    "    \n",
    "    print(\"Done processing.\")\n",
    "    print(f\"Number of workflows: {len(workflows)}\")\n",
    "    print(f\"Number of nodes: {num_nodes}\")\n",
    "    print(f\"Number of edges: {num_edges}\")\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading tool names and descriptions from the toolshed.g2.bx.psu.edu site\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import constants\n",
    "import utils\n",
    "\n",
    "\n",
    "#Description :\n",
    "#This function is used to download the tool list from the toolshed.g2.bx.psu.edu site.\n",
    "#This function will produce information for each tool consisting of information processed, \n",
    "#name, description, and downloads. The results will be saved in json form.\n",
    "def init_repositories():\n",
    "    # First, we need to download the list of repositories from the Galaxy ToolShed\n",
    "    if os.path.exists(constants.PROCESSED_REPOSITORIES):\n",
    "        processed_repositories = utils.load_json(constants.PROCESSED_REPOSITORIES)\n",
    "    else:\n",
    "        processed_repositories = {}\n",
    "    \n",
    "    with open(constants.TOOL_SOURCES) as f:\n",
    "        sources = f.read().splitlines()\n",
    "    \n",
    "    # Concatenate sources and prepare for download\n",
    "    for source in sources:\n",
    "        source_repositories_url = f\"https://{source}/api/repositories\"\n",
    "        print(\"Downloading repository list from \", source_repositories_url)\n",
    "        source_repositories = requests.get(source_repositories_url).json()\n",
    "        \n",
    "        for repository in tqdm.tqdm(source_repositories, desc=f\"Preparing {source}\"):\n",
    "            id = f\"{source} {repository['id']}\"\n",
    "            if id not in processed_repositories:\n",
    "                processed_repositories[id] = {}\n",
    "                processed_repositories[id][\"processed\"] = False\n",
    "                \n",
    "                processed_repositories[id][\"name\"] = repository[\"name\"]\n",
    "                if \"description\" in repository:\n",
    "                    processed_repositories[id][\"description\"] = repository[\"description\"]\n",
    "                processed_repositories[id][\"downloads\"] = repository[\"times_downloaded\"]\n",
    "    \n",
    "    utils.dump_json(processed_repositories, constants.PROCESSED_REPOSITORIES)\n",
    "\n",
    "\n",
    "\n",
    "#Description :\n",
    "# This function adds information for each tool from the json results\n",
    "# produced by the init_repositories function, by adding information for tools \n",
    "# consisting of id, name, profile, and other information.\n",
    "def download_repositories():\n",
    "    # Now we download all tool repositories and extract the tools from them\n",
    "    # Respecting the API wait time specified in constants\n",
    "    # See Toolshed API for structure of metadata\n",
    "    i = 0\n",
    "    prev_time = 0\n",
    "    processed_repositories = utils.load_json(constants.PROCESSED_REPOSITORIES)\n",
    "    for id, repository in processed_repositories.items():\n",
    "        i += 1\n",
    "        if repository[\"processed\"]:\n",
    "            continue\n",
    "        \n",
    "        print(\"Processing repository {}/{}: {}\".format(i, len(processed_repositories), repository[\"name\"]))\n",
    "        \n",
    "        source, actual_id = id.split(\" \")\n",
    "        metadata_url = f\"https://{source}/api/repositories/{actual_id}/metadata\"\n",
    "        \n",
    "        while (time.time() - prev_time < constants.GALAXY_API_WAIT):\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        metadata = requests.get(metadata_url).json()\n",
    "        prev_time = time.time()\n",
    "        \n",
    "        if len(metadata) == 0 or \"err_msg\" in metadata:\n",
    "            print(\"No metadata found, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get description(s) from latest version of repository\n",
    "        latest_revision = list(metadata.values())[-1]\n",
    "        if \"tools\" in latest_revision:\n",
    "            repository[\"tools\"] = latest_revision[\"tools\"]\n",
    "            for i_tool in range(len(repository[\"tools\"])):\n",
    "                tool = repository[\"tools\"][i_tool]\n",
    "                if \"tests\" in tool:\n",
    "                    del repository[\"tools\"][i_tool][\"tests\"]\n",
    "        repository[\"processed\"] = True\n",
    "        utils.dump_json(processed_repositories, constants.PROCESSED_REPOSITORIES)\n",
    "\n",
    "def run():\n",
    "    print(\"Downloading tool names and descriptions from Toolshed sources\")\n",
    "    init_repositories()\n",
    "    download_repositories()\n",
    "    print(\"Done downloading tool descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TOOL LIST\n",
    "import tqdm\n",
    "import constants\n",
    "import utils\n",
    "\n",
    "#Description :\n",
    "#This function will retrieve information from processed repositories for each tool consisting of tool descriptions and download information.\n",
    "def collect_tools_from_repos():\n",
    "    repositories = utils.load_json(constants.PROCESSED_REPOSITORIES)\n",
    "    \n",
    "    tools = {}\n",
    "    \n",
    "    for repository in tqdm.tqdm(repositories.values(), desc=\"Retrieving descriptions\"):\n",
    "        if not repository[\"processed\"]:\n",
    "            continue\n",
    "    \n",
    "        if \"tools\" not in repository:\n",
    "            continue\n",
    "        \n",
    "        # Obtain descriptions for all tools from processed repository\n",
    "        for tool in repository[\"tools\"]:\n",
    "            # If there is only one tool in the repository, often use the repository description as seen from data\n",
    "            if len(repository[\"tools\"]) == 1:\n",
    "                if tool[\"description\"] == \"\":\n",
    "                    desc = tool[\"name\"] + \". \" + repository[\"description\"]\n",
    "                else:\n",
    "                    desc = tool[\"name\"] + \" \" + tool[\"description\"] + \". \" + repository[\"description\"]\n",
    "            else:\n",
    "                desc = tool[\"name\"] + \" \" + tool[\"description\"]\n",
    "            \n",
    "            desc = desc.strip()\n",
    "            id = tool[\"id\"]\n",
    "            processed_id = id.replace(\"/\", \"_\") # Some tools have a slash in their id, but it is not a guid at this point\n",
    "            \n",
    "            # Some tools have same name, take description of one with most downloads\n",
    "            if processed_id in tools:\n",
    "                if tools[processed_id][\"downloads\"] >= repository[\"downloads\"]:\n",
    "                    continue\n",
    "            \n",
    "            tools[processed_id] = { \"description\": desc, \"downloads\": repository[\"downloads\"] }\n",
    "    \n",
    "    return tools\n",
    "\n",
    "#Description :\n",
    "#This function will add information if information for a tool with a certain ID is not available in the toolshed repository.\n",
    "def add_extra_tools(tools):\n",
    "    # Not all tools in workflows are present in Toolshed repositories, so we add them here\n",
    "    processed_workflows = utils.get_files_of_ext(constants.PROCESSED_WORKFLOWS_LOC, \"json\")\n",
    "    \n",
    "    for file in processed_workflows:\n",
    "        workflows = utils.load_json(file)\n",
    "        \n",
    "        for workflow in tqdm.tqdm(workflows, desc=\"Adding extras\"):\n",
    "            for node in workflow[\"nodes\"]:\n",
    "                id = utils.get_node_id(node)\n",
    "                \n",
    "                if id in tools:\n",
    "                    continue\n",
    "                \n",
    "                tools[id] = { \"description\": node[\"name\"], \"downloads\": 0 }\n",
    "    \n",
    "    return tools\n",
    "\n",
    "#Description :\n",
    "#this function will remove some punctuation and also reduce all words to lowercase\n",
    "\n",
    "def preprocess_descriptions(tools):\n",
    "    # Remove punctuation and make all lowercase\n",
    "    for id in tqdm.tqdm(tools, desc=\"Preprocessing descriptions\"):\n",
    "        desc = tools[id][\"description\"]\n",
    "        desc = desc.replace(\"(\", \" \").replace(\")\", \" \").replace(\",\", \" \").replace(\":\", \" \").replace(\";\", \" \").replace(\"-\", \" \").replace(\"_\", \" \").replace(\"/\", \" \").replace(\"\\\\\", \" \").replace(\"  \", \" \")\n",
    "        tools[id][\"description\"] = desc.lower().strip()\n",
    "    return tools\n",
    "\n",
    "def create_tool_list():\n",
    "    # Create toolbox from downloaded tool repositories\n",
    "    tools = collect_tools_from_repos()\n",
    "    tools = add_extra_tools(tools)\n",
    "    tools = preprocess_descriptions(tools)\n",
    "    utils.dump_json(tools, constants.TOOL_LIST)\n",
    "create_tool_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMBED Description()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import constants\n",
    "import utils\n",
    "\n",
    "\n",
    "#Description:\n",
    "#This function is used to encode the description of each tool using the PubMedBERT model and save the result\n",
    "\n",
    "def run():\n",
    "    tools = utils.load_json(constants.TOOL_LIST)\n",
    "    \n",
    "    print(\"Loading PubMedBERT model...\")\n",
    "    model = SentenceTransformer('pritamdeka/PubMedBERT-mnli-snli-scinli-scitail-mednli-stsb')\n",
    "    \n",
    "    descriptions = [tools[id][\"description\"] for id in tools]\n",
    "    \n",
    "    print(\"Embedding tool descriptions...\")\n",
    "    embeddings = model.encode(descriptions, show_progress_bar=True)\n",
    "    \n",
    "    utils.dump_numpy(embeddings, constants.DESCRIPTION_EMBEDDINGS)\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b47ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_loc = os.path.join(constants.OUT_LOC, constants.MODEL_NAME)\n",
    "optimize_base_loc = os.path.join(constants.OUT_LOC, \"{}_optimize\".format(constants.MODEL_NAME))\n",
    "base_config = {\n",
    "        \"device\": \"cuda\",\n",
    "        \"model_type\": constants.MODEL_TYPE,\n",
    "        \"hidden_channels\": 32,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"l2_penalty\": 0.00001,\n",
    "        \"step_size\": 30,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"emb_dropout\": 0.0,\n",
    "        \"dropout\": 0.0,\n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 100,\n",
    "        \"model_path\": optimize_base_loc,\n",
    "        \"model_name\": \"model.pt\",\n",
    "        \"top_k\": constants.HITRATE_K,\n",
    "        \"mrr_k\": constants.MRR_K,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data for training, testing, and validation\n",
    "import networkx as nx\n",
    "import random\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import constants\n",
    "import utils\n",
    "\n",
    "#Description :\n",
    "#perform filters on the workflow\n",
    "def filter_workflows(workflows):\n",
    "    # Filter out workflows that are too short or too long\n",
    "    return [workflow for workflow in workflows if len(workflow[\"nodes\"]) >= constants.MIN_WORKFLOW_LENGTH and len(workflow[\"nodes\"]) <= constants.MAX_WORKFLOW_LENGTH]\n",
    "\n",
    "#Description :\n",
    "#This function is used to build a list of tools in the toolbox\n",
    "def build_tool_list(workflows, toolbox):\n",
    "    tool_name_to_id = {}\n",
    "    tool_id_to_type = {}\n",
    "    next_id = 0\n",
    "    \n",
    "    # Associate each tool with a unique ID, and ensure that all tools are in the toolbox\n",
    "    for workflow in tqdm.tqdm(workflows, desc=\"Building tool list\"):\n",
    "        for node in workflow[\"nodes\"]:\n",
    "            id = utils.get_node_id(node)\n",
    "            t = utils.get_node_type(node)\n",
    "            \n",
    "            if id not in toolbox:\n",
    "                raise Exception(f\"Tool {id} not found in toolbox. Please process the toolbox first.\")\n",
    "            \n",
    "            if id not in tool_name_to_id:\n",
    "                tool_name_to_id[id] = next_id\n",
    "                tool_id_to_type[next_id] = t\n",
    "                next_id += 1\n",
    "    \n",
    "    return tool_name_to_id, tool_id_to_type\n",
    "\n",
    "#Description :\n",
    "#This function is used for Flattening tool list\n",
    "def flatten_to_type(tool_name_to_id, tool_id_to_type, types=[\"tool\"]):\n",
    "    # Remove nodes that are not of the specified types, generally all but tools\n",
    "    tool_name_to_id_flattened = {}\n",
    "    tool_id_to_type_flattened = {}\n",
    "    next_id = 0\n",
    "    \n",
    "    # Also flatten the associated dictionaries\n",
    "    for tool_name, tool_id in tqdm.tqdm(tool_name_to_id.items(), desc=\"Flattening tool list\"):\n",
    "        if tool_id_to_type[tool_id] in types:\n",
    "            tool_name_to_id_flattened[tool_name] = next_id\n",
    "            tool_id_to_type_flattened[next_id] = tool_id_to_type[tool_id]\n",
    "            next_id += 1\n",
    "    \n",
    "    return tool_name_to_id_flattened, tool_id_to_type_flattened\n",
    "\n",
    "\n",
    "#Description :\n",
    "#This function creates a directed graph for the workflow, with nodes representing tools and edges representing connections between tools.\n",
    "def build_workflow_dag(workflow, tool_name_to_id, toolbox, types=[\"tool\"]):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Map nodes with tool IDs and description embeddings\n",
    "    node_index = 0\n",
    "    for node in workflow[\"nodes\"]:\n",
    "        tool_type = utils.get_node_type(node)\n",
    "        if tool_type not in types:\n",
    "            node_index += 1\n",
    "            continue\n",
    "        \n",
    "        tool_name = utils.get_node_id(node)\n",
    "        G.add_node(node_index, tool_id=tool_name_to_id[tool_name], tool_type=tool_type, embedding=toolbox[tool_name][\"embedding\"])\n",
    "        node_index += 1\n",
    "    \n",
    "    # Map edges between nodes\n",
    "    for from_node in workflow[\"edges\"]:\n",
    "        from_node_int = int(from_node) # JSON keys are strings\n",
    "        for to_node in workflow[\"edges\"][from_node]:\n",
    "            to_node_string = str(to_node) # JSON values are ints\n",
    "            if utils.get_node_type(workflow[\"nodes\"][from_node_int]) not in types:\n",
    "                continue\n",
    "            if utils.get_node_type(workflow[\"nodes\"][to_node]) not in types:\n",
    "                # Check if bypass edge should be added\n",
    "                for to_node2 in workflow[\"edges\"][to_node_string]:\n",
    "                    to_node2\n",
    "                    if utils.get_node_type(workflow[\"nodes\"][to_node2]) not in types:\n",
    "                        continue\n",
    "                    G.add_edge(from_node_int, to_node2)\n",
    "            else:\n",
    "                G.add_edge(from_node_int, to_node)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "#Description :\n",
    "#This function creates a directed graph for the workflow\n",
    "def build_unique_workflow_dags(workflows, tool_name_to_id, toolbox, types=[\"tool\"]):\n",
    "    # Use graph hash to filter out duplicates\n",
    "    graph_hashes = []\n",
    "    unique_graph_dicts = []\n",
    "    \n",
    "    for workflow in tqdm.tqdm(workflows, desc=\"Building workflow DAGs\"):\n",
    "        G = build_workflow_dag(workflow, tool_name_to_id, toolbox, types)\n",
    "        graph_hash = nx.weisfeiler_lehman_graph_hash(G, node_attr=\"tool_id\") # Strong guarantee of uniqueness between non-isomorphic graphs\n",
    "        \n",
    "        # Only keep unique graphs\n",
    "        if graph_hash not in graph_hashes:\n",
    "            graph_hashes.append(graph_hash)\n",
    "            graph_dict = { \"graph\": G }\n",
    "            \n",
    "            # Other information that can help inspection of results, not present in EuGalaxy dataset\n",
    "            if \"name\" in workflow:\n",
    "                graph_dict[\"name\"] = workflow[\"name\"]\n",
    "            if \"server\" in workflow:\n",
    "                graph_dict[\"server\"] = workflow[\"server\"]\n",
    "            if \"workflow_id\" in workflow:\n",
    "                graph_dict[\"workflow_id\"] = workflow[\"workflow_id\"]\n",
    "            unique_graph_dicts.append(graph_dict)\n",
    "    return unique_graph_dicts\n",
    "\n",
    "\n",
    "#Description :\n",
    "#to get the sequence of the graph\n",
    "def get_all_dag_paths(G):\n",
    "    # Get all linear sequences in a DAG from all sources to all sinks\n",
    "    sources = [node for node in G.nodes if G.in_degree(node) == 0]\n",
    "    sinks = [node for node in G.nodes if G.out_degree(node) == 0]\n",
    "    \n",
    "    paths = []\n",
    "    for source in sources:\n",
    "        for sink in sinks:\n",
    "            paths.extend(nx.all_simple_paths(G, source, sink))\n",
    "    \n",
    "    return paths\n",
    "\n",
    "\n",
    "#Description :\n",
    "#to build workflow path sequence with the number is at least 2\n",
    "def build_workflow_paths_sequence(workflow_dict):\n",
    "    G = workflow_dict[\"graph\"]\n",
    "    paths = get_all_dag_paths(G)\n",
    "    path_sequences = []\n",
    "    for path in paths:\n",
    "        path_sequence = []\n",
    "        \n",
    "        # Skip paths that are too short to have an input and ground truth\n",
    "        if len(path) < 2:\n",
    "            continue\n",
    "        \n",
    "        for node in path:\n",
    "            path_sequence.append((G.nodes[node][\"tool_id\"], G.nodes[node][\"embedding\"]))\n",
    "        \n",
    "        path_sequences.append(path_sequence)\n",
    "    \n",
    "    return path_sequences\n",
    "#Description :\n",
    "#to split workflow data into train data, validation data, and testing data\n",
    "def split_data(workflow_dicts):\n",
    "    # Split workflows randomly into train, test, val splits as specified in constants\n",
    "    seed = random.randint(0, 1000000)\n",
    "    workflow_ids = list(range(len(workflow_dicts)))\n",
    "    random.Random(seed).shuffle(workflow_ids)\n",
    "    \n",
    "    train_ids = workflow_ids[:int(len(workflow_ids) * constants.TRAIN_SPLIT)]\n",
    "    val_ids = workflow_ids[int(len(workflow_ids) * constants.TRAIN_SPLIT):int(len(workflow_ids) * (constants.TRAIN_SPLIT + constants.VAL_SPLIT))]\n",
    "    test_ids = workflow_ids[int(len(workflow_ids) * (constants.TRAIN_SPLIT + constants.VAL_SPLIT)):]\n",
    "    \n",
    "    train_data = [workflow_dicts[i] for i in train_ids]\n",
    "    val_data = [workflow_dicts[i] for i in val_ids]\n",
    "    test_data = [workflow_dicts[i] for i in test_ids]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def get_partial_graphs(G):\n",
    "    # Get all sub-graphs for a given graph, where each sub-graph is the reverse DFS tree of a node\n",
    "    partial_graphs = []\n",
    "    for node in G.nodes:\n",
    "        # Use reverse DFS to get partial graphs\n",
    "        if G.in_degree(node) == 0:\n",
    "            continue\n",
    "        \n",
    "        reverse_dfs_tree = nx.dfs_tree(G.reverse(), node).reverse()\n",
    "        reverse_dfs_tree.add_nodes_from((n, G.nodes[n]) for n in reverse_dfs_tree.nodes)\n",
    "        reverse_dfs_tree.remove_node(node) # This is the ground truth, which will become masked node at \"recommendation position\".\n",
    "        \n",
    "        partial_graphs.append({ \"graph\": reverse_dfs_tree, \"y\": G.nodes[node][\"tool_id\"] })\n",
    "    \n",
    "    return partial_graphs\n",
    "\n",
    "def get_partial_paths(path_sequences):\n",
    "    # Get all partial paths for a given path sequence, where each partial path is a sub-sequence of the path length 2 to n\n",
    "    partial_paths = []\n",
    "    unique_paths = []\n",
    "    for path in path_sequences:\n",
    "        for i in range(2, (len(path) + 1)):\n",
    "            path_tuple = tuple([x[0] for x in path[:i]])\n",
    "            if path_tuple not in unique_paths:\n",
    "                unique_paths.append(path_tuple)\n",
    "                partial_paths.append(path[:i])\n",
    "    \n",
    "    return partial_paths\n",
    "\n",
    "#Description :\n",
    "#to prepare data for training, testing and validation.\n",
    "def prepare(workflows_path, data_output_path):\n",
    "    # Driver function to prepare data for models\n",
    "    \n",
    "    # Load workflows and toolbox\n",
    "    print(\"Preparing data for models...\")\n",
    "    workflows = utils.load_json(workflows_path)\n",
    "    toolbox, embedding_size = utils.load_toolbox()\n",
    "    \n",
    "    # Filter workflows and build tool list\n",
    "    print(\"Length of workflows before filtering: {}\".format(len(workflows)))\n",
    "    workflows = filter_workflows(workflows)\n",
    "    print(\"Length of workflows after filtering: {}\".format(len(workflows)))\n",
    "    tool_name_to_id, tool_id_to_type = build_tool_list(workflows, toolbox)\n",
    "    tool_name_to_id, tool_id_to_type = flatten_to_type(tool_name_to_id, tool_id_to_type)\n",
    "    \n",
    "    num_tools = len(tool_name_to_id)\n",
    "    \n",
    "    # Build unique workflow DAGs\n",
    "    workflow_dicts = build_unique_workflow_dags(workflows, tool_name_to_id, toolbox)\n",
    "    \n",
    "    # Build paths for each workflow DAG\n",
    "    for workflow_dict in workflow_dicts:\n",
    "        workflow_dict[\"path_sequences\"] = build_workflow_paths_sequence(workflow_dict)\n",
    "    \n",
    "    # Split data into train, val, test\n",
    "    train_data, val_data, test_data = split_data(workflow_dicts)\n",
    "    \n",
    "    # Build partial graphs and paths for each workflow DAG\n",
    "    for data in [train_data, val_data, test_data]:\n",
    "        for workflow_dict in tqdm.tqdm(data, desc=\"Building partial graphs and paths\"):\n",
    "            workflow_dict[\"partial_graphs\"] = get_partial_graphs(workflow_dict[\"graph\"])\n",
    "            workflow_dict[\"partial_paths\"] = get_partial_paths(workflow_dict[\"path_sequences\"])\n",
    "    \n",
    "    # Save data\n",
    "    raw_output_path = os.path.join(data_output_path, \"raw\")\n",
    "    print(\"Saving data...\")\n",
    "    utils.dump_pickle(train_data, os.path.join(raw_output_path, \"train_data.pickle\"))\n",
    "    utils.dump_pickle(val_data, os.path.join(raw_output_path, \"val_data.pickle\"))\n",
    "    utils.dump_pickle(test_data, os.path.join(raw_output_path, \"test_data.pickle\"))\n",
    "    \n",
    "    info = { \"num_tools\": num_tools, \"tool_name_to_id\": tool_name_to_id, \"embedding_size\": embedding_size }\n",
    "    utils.dump_json(info, os.path.join(data_output_path, \"info.json\"))\n",
    "    \n",
    "    print(\"Done processing.\")\n",
    "\n",
    "    \n",
    "def prepare_data_splits(model_base_loc, optimize_base_loc):\n",
    "    ### PREPARE DATA AND SPLITS\n",
    "    prepare_data.prepare(os.path.join(constants.PROCESSED_WORKFLOWS_LOC, \"{}.json\".format(constants.MODEL_DATA)), model_base_loc)\n",
    "    prepare_data.prepare(os.path.join(constants.PROCESSED_WORKFLOWS_LOC, \"{}.json\".format(constants.MODEL_DATA)), optimize_base_loc)\n",
    "\n",
    "    \n",
    "model_base_loc = os.path.join(constants.OUT_LOC, constants.MODEL_NAME)\n",
    "optimize_base_loc = os.path.join(constants.OUT_LOC, \"{}_optimize\".format(constants.MODEL_NAME))\n",
    "\n",
    "prepare_data_splits(model_base_loc, optimize_base_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5672522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZATION FOR HYPERPARAMETER\n",
    "import hyperopt\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import data_loader\n",
    "import constants\n",
    "import model\n",
    "\n",
    "i = 0\n",
    "#Description :\n",
    "#to optimize hyperparameter dengan Bayesian optimization, to find the set of hyperparameters that minimizes loss.\n",
    "def optimize(config, ranges, num_evals, output_name=\"best_hyperparameters.json\"):\n",
    "    params = config.copy()\n",
    "    \n",
    "    # Define hyperparaments and ranges\n",
    "    params[\"hidden_channels\"] = hyperopt.hp.quniform(\"hidden_channels\", ranges[\"hidden_channels\"][0], ranges[\"hidden_channels\"][1], 1)\n",
    "    params[\"learning_rate\"] = hyperopt.hp.loguniform(\"learning_rate\", np.log(ranges[\"learning_rate\"][0]), np.log(ranges[\"learning_rate\"][1]))\n",
    "    params[\"l2_penalty\"] = hyperopt.hp.loguniform(\"l2_penalty\", np.log(ranges[\"l2_penalty\"][0]), np.log(ranges[\"l2_penalty\"][1]))\n",
    "    params[\"step_size\"] = hyperopt.hp.quniform(\"step_size\", ranges[\"step_size\"][0], ranges[\"step_size\"][1], 1)\n",
    "    params[\"emb_dropout\"] = hyperopt.hp.uniform(\"emb_dropout\", ranges[\"emb_dropout\"][0], ranges[\"emb_dropout\"][1])\n",
    "    params[\"dropout\"] = hyperopt.hp.uniform(\"dropout\", ranges[\"dropout\"][0], ranges[\"dropout\"][1])\n",
    "    params[\"batch_size\"] = hyperopt.hp.quniform(\"batch_size\", ranges[\"batch_size\"][0], ranges[\"batch_size\"][1], 1)\n",
    "    params[\"epochs\"] = hyperopt.hp.quniform(\"epochs\", ranges[\"epochs\"][0], ranges[\"epochs\"][1], 1)\n",
    "    \n",
    "    def objective(params):\n",
    "        global i\n",
    "        i += 1\n",
    "        model_config = params.copy()\n",
    "        \n",
    "        # Convert some parameters back to int\n",
    "        model_config[\"hidden_channels\"] = int(model_config[\"hidden_channels\"])\n",
    "        model_config[\"step_size\"] = int(model_config[\"step_size\"])\n",
    "        model_config[\"batch_size\"] = int(model_config[\"batch_size\"])\n",
    "        model_config[\"epochs\"] = int(model_config[\"epochs\"])\n",
    "        \n",
    "        print(\"Hyperparameter Optimize Iteration: {}\".format(i))\n",
    "        print(\"Hyperparameters: {}\".format(model_config))\n",
    "        \n",
    "        model_config = data_loader.add_data_config(model_config)\n",
    "        best_model, best_epoch, best_acc, val_accs, losses = model.train_model(model_config, use_tqdm=False)\n",
    "        return min(losses)\n",
    "    \n",
    "    trials = hyperopt.Trials()\n",
    "    best = hyperopt.fmin(objective, params, algo=hyperopt.tpe.suggest, max_evals=num_evals, trials=trials)\n",
    "    utils.dump_json(best, os.path.join(config[\"model_path\"], output_name))\n",
    "    \n",
    "    return best\n",
    "def optimize_hyperparameters(base_config):\n",
    "    ## OPTIMIZE HYPERPARAMETERS OVER 10 ITERATIONS\n",
    "    ranges = {\n",
    "        \"hidden_channels\": [16, 64],\n",
    "        \"learning_rate\": [0.0001, 0.01],\n",
    "        \"l2_penalty\": [0.00001, 0.01],\n",
    "        \"step_size\": [10, 30],\n",
    "        \"emb_dropout\": [0.0, 0.5],\n",
    "        \"dropout\": [0.0, 0.5],\n",
    "        \"batch_size\": [32, 128],\n",
    "        \"epochs\": [50, 100],\n",
    "    }\n",
    "    \n",
    "    num_evals = constants.NUM_OPTIMIZE_ITERATIONS\n",
    "    optimize_hyper.optimize(base_config, ranges, num_evals, \"best_hyperparameters.json\")\n",
    "optimize_hyperparameters(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1461b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL TRAINING\n",
    "\n",
    "#Description :\n",
    "#This function is used to train the model using the best hyperparameters obtained from the previous stage.\n",
    "def train_model(base_config, model_base_loc, optimize_base_loc):\n",
    "    ## TRAIN ONE MODEL\n",
    "    config = base_config.copy()\n",
    "    best_params = utils.load_json('best_hyperparameters.json')\n",
    "    \n",
    "    for key, value in best_params.items():\n",
    "        config[key] = value\n",
    "    \n",
    "    config[\"hidden_channels\"] = int(config[\"hidden_channels\"])\n",
    "    config[\"step_size\"] = int(config[\"step_size\"])\n",
    "    config[\"batch_size\"] = int(config[\"batch_size\"])\n",
    "    config[\"epochs\"] = int(config[\"epochs\"])\n",
    "    config[\"model_path\"] = model_base_loc\n",
    "    config = data_loader.add_data_config(config)\n",
    "    \n",
    "    best_model, best_epoch, best_acc, val_accs, losses = model.train_model(config)\n",
    "    print(\"Best Epoch: {}, Best Val Acc: {}\".format(best_epoch, best_acc))\n",
    "    model.save_model(best_model, config)\n",
    "train_model(base_config, model_base_loc, optimize_base_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##MODEL Testing\n",
    "\n",
    "#Description :\n",
    "#This function is used to test the model, and will calculate the accuracy metrics HR1, HR2, and MRR.\n",
    "def test_model(base_config, model_base_loc, optimize_base_loc):\n",
    "    # TEST ONE MODEL\n",
    "    config = base_config.copy()\n",
    "    best_params = utils.load_json(os.path.join(optimize_base_loc, \"best_hyperparameters.json\"))\n",
    "    \n",
    "    for key, value in best_params.items():\n",
    "        config[key] = value\n",
    "    \n",
    "    config[\"hidden_channels\"] = int(config[\"hidden_channels\"])\n",
    "    config[\"step_size\"] = int(config[\"step_size\"])\n",
    "    config[\"batch_size\"] = int(config[\"batch_size\"])\n",
    "    config[\"epochs\"] = int(config[\"epochs\"])\n",
    "    config[\"model_path\"] = model_base_loc\n",
    "    config = data_loader.add_data_config(config)\n",
    "    \n",
    "    if config[\"model_type\"] == \"graph\":\n",
    "        test_dataset = data_loader.GraphDataset(root=config[\"model_path\"], name=\"test_data\")\n",
    "    else:\n",
    "        test_dataset = data_loader.PathDataset(root=config[\"model_path\"], name=\"test_data\")\n",
    "    \n",
    "    test_loader = torch_geometric.loader.DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    best_model = model.load_model(config)\n",
    "    acc, top_k, mrr_k, by_length = model.evaluate_model(best_model, test_loader, config)\n",
    "    print(\"HR1: {}, HR3: {}, MRR: {}\".format(acc*100, top_k*100, mrr_k*100))\n",
    "test_model(base_config, model_base_loc, optimize_base_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bdbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE RECOMMENDATION\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "import constants\n",
    "import model\n",
    "import utils\n",
    "import data_loader\n",
    "\n",
    "\n",
    "#Description :\n",
    "#This function is used to processes a graph path, associates embeddings with nodes.\n",
    "def path_to_data(path):\n",
    "    x = path\n",
    "    \n",
    "    steps = [node[0] for node in x]\n",
    "    id_to_embedding = {node[0]: node[1] for node in x}\n",
    "    indices, x = pd.factorize(steps)\n",
    "    \n",
    "    senders, receivers = indices[:-1], indices[1:]\n",
    "    \n",
    "    # Associate embedding with tool instance in sequence\n",
    "    full_x = []\n",
    "    for item in x:\n",
    "        seq = [item]\n",
    "        seq.extend(id_to_embedding[item])\n",
    "        full_x.append(seq)\n",
    "    \n",
    "    full_x = torch.tensor(full_x, dtype=torch.float)\n",
    "    edge_index = torch.tensor([senders, receivers], dtype=torch.long)\n",
    "\n",
    "    data = torch_geometric.data.Batch.from_data_list([torch_geometric.data.Data(x=full_x, edge_index=edge_index)])\n",
    "    \n",
    "    return data\n",
    "\n",
    "#Description :\n",
    "#This function is used to evaluate model on some input data\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data)\n",
    "        return logits\n",
    "\n",
    "#Description :\n",
    "#check whether each element in the sequence entered by the user is recognized by the tool name list.\n",
    "#If it is not recognized, it will recommend 10 other tools that are similar to that element.\n",
    "def validate_sequence(input_sequence, all_tool_names):\n",
    "    sequence = [tool.strip() for tool in input_sequence]\n",
    "    \n",
    "    if len(sequence) == 0:\n",
    "        print(\"No sequence provided\")\n",
    "        return []\n",
    "    \n",
    "    true_sequence = []\n",
    "    for tool in sequence:\n",
    "        if tool in all_tool_names:\n",
    "            true_sequence.append(tool)\n",
    "        else:\n",
    "            distances = {}\n",
    "            for name in all_tool_names:\n",
    "                distances[name] = fuzz.ratio(name, tool)\n",
    "            \n",
    "            sorted_candidates = sorted(distances.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"Tool {tool} not found in workflow, showing {constants.CANDIDATES_TO_SHOW} potential matches:\")\n",
    "            for i in range(constants.CANDIDATES_TO_SHOW):\n",
    "                print(f\"{i + 1}. {sorted_candidates[i][0]}\")\n",
    "            \n",
    "            if (constants.MATCH_INTERACTIVELY):\n",
    "                print(\"0 to exit\")\n",
    "                print(\"Please select a tool to use:\")\n",
    "                \n",
    "                while True:\n",
    "                    try:\n",
    "                        selection = int(input())\n",
    "                        if selection == 0:\n",
    "                            return []\n",
    "                        elif selection > 0 and selection <= constants.CANDIDATES_TO_SHOW:\n",
    "                            true_sequence.append(sorted_candidates[selection - 1][0])\n",
    "                            break\n",
    "                    except:\n",
    "                        print(\"Invalid input, please try again\")\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "#Description:\n",
    "#To convert sequence entered by the user\n",
    "def convert_sequence(sequence, tool_name_to_id, toolbox):\n",
    "    converted_sequence = []\n",
    "    for tool in sequence:\n",
    "        embedding = toolbox[tool][\"embedding\"]\n",
    "        converted_sequence.append((tool_name_to_id[tool], embedding))\n",
    "    \n",
    "    return converted_sequence\n",
    "\n",
    "def get_recommendation(base_config, model_base_loc, optimize_base_loc, input_sequence):\n",
    "    info = utils.load_json(os.path.join(model_base_loc, \"info.json\"))\n",
    "    tool_name_to_id = info[\"tool_name_to_id\"]\n",
    "    all_tool_names = list(tool_name_to_id.keys())\n",
    "    sequence  = input_sequence\n",
    "    sequence = validate_sequence(input_sequence, all_tool_names)\n",
    "    \n",
    "    if len(sequence) == 0:\n",
    "        return []\n",
    "    \n",
    "    config = base_config.copy()\n",
    "    best_params = utils.load_json(os.path.join(optimize_base_loc, \"best_hyperparameters.json\"))\n",
    "    \n",
    "    for key, value in best_params.items():\n",
    "        config[key] = value\n",
    "    \n",
    "    config[\"hidden_channels\"] = int(config[\"hidden_channels\"])\n",
    "    config[\"step_size\"] = int(config[\"step_size\"])\n",
    "    config[\"batch_size\"] = int(config[\"batch_size\"])\n",
    "    config[\"epochs\"] = int(config[\"epochs\"])\n",
    "    config[\"model_path\"] = model_base_loc\n",
    "    config = data_loader.add_data_config(config)\n",
    "    \n",
    "    toolbox, _ = utils.load_toolbox()\n",
    "    \n",
    "    id_sequence = convert_sequence(sequence, tool_name_to_id, toolbox)\n",
    "    data = path_to_data(id_sequence)\n",
    "    loaded_model = model.load_model(config)\n",
    "    \n",
    "    logits = predict(loaded_model.to('cuda'), data.to('cuda'))\n",
    "    recommendations = torch.topk(logits, constants.TOOLS_TO_RECOMMEND)[1].view(-1).cpu().detach().numpy()\n",
    "    \n",
    "    tool_id_to_name = { v: k for k, v in tool_name_to_id.items() }\n",
    "    recommended_tools = [tool_id_to_name[tool_id] for tool_id in recommendations]\n",
    "    \n",
    "    return recommended_tools\n",
    "\n",
    "model_base_loc = os.path.join(constants.OUT_LOC, constants.MODEL_NAME)\n",
    "optimize_base_loc = os.path.join(constants.OUT_LOC, \"{}_optimize\".format(constants.MODEL_NAME))\n",
    "\n",
    "base_config = {\n",
    "    \"device\": \"cuda\",\n",
    "    \"model_type\": constants.MODEL_TYPE,\n",
    "    \"hidden_channels\": 32,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"l2_penalty\": 0.00001,\n",
    "    \"step_size\": 30,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"emb_dropout\": 0.0,\n",
    "    \"dropout\": 0.0,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 100,\n",
    "    \"model_path\": optimize_base_loc,\n",
    "    \"model_name\": \"model.pt\",\n",
    "    \"top_k\": constants.HITRATE_K,\n",
    "    \"mrr_k\": constants.MRR_K,\n",
    "}\n",
    "\n",
    "\n",
    "recommended_tools=get_recommendation(base_config, model_base_loc, optimize_base_loc, ['umi_tools_extract','rna_star','bamFilter'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
